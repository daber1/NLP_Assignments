{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvig’s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bigrams = pd.read_csv('bigrams.txt', sep='\\t', encoding='ansi', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.rename(columns={0:'freq',1:'prev_word',2:'word'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivegrams = pd.read_csv('fivegrams.txt', sep='\\t', header=None)\n",
    "fivegrams.rename(columns={0:'freq',1:'prev_word4',2:'prev_word3',3:'prev_word2',4:'prev_word',5:'word'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter(words(open('big.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "MoQeEsZvHvvi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "\n",
    "class ContextSpellCorrector:\n",
    "    def __init__(self, unigrams, bigrams, fivegrams, alpha=1, beta=0.2, gamma=0.1, delta=0.5, sigma=0.5):\n",
    "        self.bigrams = bigrams\n",
    "        self.fivegrams = fivegrams\n",
    "        self.unigrams = unigrams\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def P_unigram(self, word):\n",
    "        N = sum(self.unigrams.values())\n",
    "        return unigrams[word] / N\n",
    "    \n",
    "    def P_bigram_inv(self, word, next_word_bigram):\n",
    "        next_word_bigram = next_word_bigram[-1]\n",
    "        bigram_word_count = self.bigrams[(self.bigrams.word==next_word_bigram) & (self.bigrams.prev_word==word)].freq.sum()+1\n",
    "        bigram_next_word_count = self.bigrams[self.bigrams.word==next_word_bigram].freq.sum()+1\n",
    "        return bigram_word_count / bigram_next_word_count\n",
    "\n",
    "    def P_bigram(self, word, prev_word_bigram):\n",
    "        prev_word_bigram = prev_word_bigram[-1]\n",
    "        bigram_word_count = self.bigrams[(self.bigrams.prev_word==prev_word_bigram) &(self.bigrams.word==word)].freq.sum()+1\n",
    "        bigram_prev_word_count = self.bigrams[self.bigrams.prev_word==prev_word_bigram].freq.sum()+1\n",
    "        return bigram_word_count / bigram_prev_word_count\n",
    "\n",
    "    def P_fivegram(self, word, prev_words):\n",
    "        # prev_words - 4 words\n",
    "        prev_word, prev_word2, prev_word3, prev_word4 = prev_words[-1],prev_words[-2],prev_words[-3],prev_words[-4]\n",
    "        \n",
    "        fivegram_word_count = self.fivegrams[(self.fivegrams.prev_word==prev_word)&(self.fivegrams.prev_word2==prev_word2)\n",
    "                                             & (self.fivegrams.prev_word3==prev_word3)\n",
    "                                             & (self.fivegrams.prev_word4==prev_word4)\n",
    "                                             & (self.fivegrams.word==word)].freq.sum()+1\n",
    "        fivegram_prev_word_count = self.fivegrams[(self.fivegrams.prev_word==prev_word)&(self.fivegrams.prev_word2==prev_word2)\n",
    "                                             & (self.fivegrams.prev_word3==prev_word3)\n",
    "                                             & (self.fivegrams.prev_word4==prev_word4)].freq.sum()+1\n",
    "        return fivegram_word_count / fivegram_prev_word_count\n",
    "\n",
    "    def correct(self, word, prev_words=None, future_word=None, threshold=0.5, debug=False):\n",
    "        candidates = self.generate_candidates(word)\n",
    "        if len(prev_words)==5:\n",
    "            for i in range(len(candidates)):\n",
    "                candidates[i] = [candidates[i], self.alpha*self.P_unigram(candidates[i])+self.gamma*self.P_fivegram(candidates[i],prev_words)+self.beta*self.P_bigram(candidates[i], prev_words)]\n",
    "        elif 1<=len(prev_words)<5:\n",
    "            for i in range(len(candidates)):\n",
    "                candidates[i] = [candidates[i], self.alpha*self.P_unigram(candidates[i])+self.beta*self.P_bigram(candidates[i],prev_words)]\n",
    "        else:\n",
    "            for i in range(len(candidates)):\n",
    "                candidates[i] = [candidates[i],1/len(candidates)]\n",
    "        \n",
    "        if future_word:\n",
    "            for i in range(len(candidates)):\n",
    "                prob_inv = self.P_bigram_inv(candidates[i][0], future_word)\n",
    "                if prob_inv != 1:\n",
    "                    candidates[i][1] = self.delta*candidates[i][1] + self.sigma*prob_inv\n",
    "#                 candidates_inv.append((candidates[i][0], ))\n",
    "#             candidates_inv.sort(reverse=True, key=lambda x:x[-1])\n",
    "#             if len(candidates_inv)==1:\n",
    "#                 best_candidate_inv = candidates_inv[0]\n",
    "#             else:\n",
    "#                 if candidates_inv[0][-1]==candidates_inv[1][-1]:\n",
    "                    # we have candidates with similar probs\n",
    "#                     similar_candidates = [cand for cand in candidates_inv if cand[-1]==candidates_inv[0][-1]]\n",
    "#                     best_candidate_inv = random.choice(similar_candidates)\n",
    "#                 else:\n",
    "                    # we have only one best candidate\n",
    "#                     best_candidate_inv =  candidates_inv[0]\n",
    "#             if (best_candidate_inv[1]>threshold) and (best_candidate_inv[1]!=1):\n",
    "#                 return best_candidate_inv[0]\n",
    "        candidates.sort(reverse=True, key=lambda x: x[-1])\n",
    "        if debug:\n",
    "            print(candidates)\n",
    "        if len(candidates)==1:\n",
    "            return candidates[0][0]\n",
    "        else:\n",
    "            if candidates[0][-1]==candidates[1][-1]:\n",
    "                # we have candidates with similar probs\n",
    "                similar_candidates = [cand[0] for cand in candidates if cand[-1]==candidates[0][-1]]\n",
    "                return random.choice(similar_candidates)\n",
    "            else:\n",
    "                # we have only one best candidate\n",
    "                return candidates[0][0]\n",
    "\n",
    "    def generate_candidates(self, word):\n",
    "        # Generate candidate corrections using edit distance \n",
    "        spell_checker = nltk.corpus.words.words()\n",
    "        candidates = set()\n",
    "        if word in spell_checker:\n",
    "            return [word]\n",
    "        # insert a char\n",
    "        for i in range(len(word)+1):\n",
    "            for c in string.ascii_lowercase:\n",
    "                candidate = word[:i] + c + word[i:]\n",
    "                if candidate in spell_checker:\n",
    "                    candidates.add(candidate)\n",
    "                \n",
    "        # delete a char\n",
    "        for i in range(len(word)):\n",
    "            candidate = word[:i] + word[i+1:]\n",
    "            if candidate in spell_checker:\n",
    "                candidates.add(candidate)\n",
    "        # substitute a char\n",
    "        for i in range(len(word)):\n",
    "            for c in string.ascii_lowercase:\n",
    "                candidate = word[:i] + c + word[i+1:]\n",
    "                if candidate in spell_checker:\n",
    "                    candidates.add(candidate)\n",
    "        if len(candidates)==0:\n",
    "            return [word]\n",
    "        return list(candidates)\n",
    "\n",
    "corrector = ContextSpellCorrector(unigrams, bigrams, fivegrams)\n",
    "\n",
    "def correct_sentence(sentence, debug=False):\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        prev_words = words[:i]\n",
    "        future_words = words[i+1:]\n",
    "        corrected_word = corrector.correct(word, prev_words, future_words, debug)\n",
    "#         print(corrected_word)\n",
    "        words[i] = corrected_word\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "<b>The reasons for my implementation:</b>\n",
    "1. The approach uses n-gram models to estimate the probability of a word given its context. It allows the spell checker to consider not only the current word but also its surrounding words.\n",
    "2. Candidates are generated using edit distances applied to the misspelled word.\n",
    "3. The spell corrector takes into account both previous and next words when choosing the best correction. It was made to fix such issues as \"dking sport\" vs \"dking species\".\n",
    "4. In cases when multiple candidates have same probabilities, the algorithm chooses the random one, so that it is not stuck with the same correct each run.\n",
    "\n",
    "<b>Problems I have encountered:</b>\n",
    "1. I haven't managed to find a good dataset of ngrams for bigrams_inverse approach. Because of this, the algorithm may give wrong corrections. For example, as the bigram dataset doesn't contain a bigram 'doing sport', it won't be able always predict this correction, but instead will randomize the correction for 'dking'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": [
    "with open('sentences.txt','r') as f:\n",
    "    sentences = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence.replace('вЂ™',\"'\").replace('.','') for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "def misspell_word(word):\n",
    "    candidates = set()\n",
    "    for i in range(len(word) + 1):\n",
    "        for c in string.ascii_lowercase:\n",
    "            candidate = word[:i] + c + word[i:]\n",
    "            candidates.add(candidate)\n",
    "    for i in range(len(word)):\n",
    "        candidate = word[:i] + word[i + 1:]\n",
    "        candidates.add(candidate)\n",
    "    for i in range(len(word)):\n",
    "        for c in string.ascii_lowercase:\n",
    "            candidate = word[:i] + c + word[i + 1:]\n",
    "            candidates.add(candidate)\n",
    "    return random.choice(list(candidates))\n",
    "def generate_test_set(dataset, noise):\n",
    "    test_set = []\n",
    "    for i in range(len(dataset)):\n",
    "        # Noise is how many words in a sentence require correction\n",
    "        original_sent = dataset[i]\n",
    "        sent = dataset[i].split()\n",
    "        n = len(sent)\n",
    "        n_corrections = ceil(n*noise)\n",
    "        indices_to_change = random.sample(range(len(sent)),k=n_corrections)\n",
    "        for j in indices_to_change:\n",
    "            sent[j] = misspell_word(sent[j])\n",
    "        test_set.append((' '.join(sent),original_sent))\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_set):\n",
    "    my_score = 0\n",
    "    norvig_score = 0\n",
    "    for sample in tqdm(test_set):\n",
    "        sent_to_fix, original_sent = sample\n",
    "        # My algorithm\n",
    "        my_sent = correct_sentence(sent_to_fix)\n",
    "        # Norvig\n",
    "        norvig_sent = [correction(w) for w in sent_to_fix.split()]\n",
    "        my_score += sum(1 for a, b in zip(my_sent.split(), sent_to_fix.split()) if a == b)/len(sent_to_fix.split())\n",
    "        norvig_score += sum(1 for a, b in zip(norvig_sent, sent_to_fix.split()) if a == b)/len(sent_to_fix.split())\n",
    "\n",
    "    return my_score/len(test_set), norvig_score/len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [18:45<00:00, 11.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My accuracy:0.648727771411595\t Norvig's accuracy:0.7106522329150036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [25:24<00:00, 15.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My accuracy:0.5558984135489553\t Norvig's accuracy:0.6093306944173819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [32:46<00:00, 19.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My accuracy:0.44964906189441806\t Norvig's accuracy:0.45677017134911874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "noises = [0.1,0.25,0.5]\n",
    "for noise in noises:\n",
    "    test_set = generate_test_set(sentences[:100], noise)\n",
    "    my_acc,norvig_acc = evaluate(test_set)\n",
    "    print(f\"My accuracy:{my_acc}\\t Norvig's accuracy:{norvig_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Accuracy:</b>\n",
    "* In the experiments, Norvig's corrector generally outperformed my approach.\n",
    "* The difference between algorithms is that my checker utilizes n-grams to predict the correction and also considers the future context, while Norvig's corrector only uses word frequencies\n",
    "\n",
    "<b>2. Flexibility:</b>\n",
    "* An advantage of my approach is its tunable hyperparamters. We can adjust the following parameters: 'alpha', 'beta', 'gamma', 'delta' and 'sigma'. This can be used to improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
